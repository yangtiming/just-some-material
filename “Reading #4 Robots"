9  Responsibility for Military Robots
Gert-Jan Lokhorst and Jeroen van den Hoven

Several  authors  have  argued  that  it  is  unethical  to  deploy  autonomous  artiﬁcially intelligent robots in warfare. They have proposed two main reasons for making this claim. First, they maintain that it is immoral to deploy such robots because such robots are  “killer  robots.”  Second,  they  claim  that  such  robots  cannot be  held  responsible because  they  cannot  suffer,  and  therefore  cannot  be  punished.  We  object  to  both claims. We  ﬁrst  point  out  that  military  robots  are  not  necessarily  killer  robots,  and that, even if they were, their behavior could still be ethically correct—it could even be preferable to the behavior of human soldiers  (section 9.1). Second, we argue that responsibility is not essentially related to punishment (section 9.2). Third, we propose an alternative analysis of responsibility, according to which robots could be respon- sible for their actions, at least to a certain extent (section 9.3). Finally, we emphasize that the primary responsibility for the behavior of military robots is in the hands of those who design and deploy them (sections 9.4 and 9.5).

9.1   Killer Robots

Sparrow (2007) and Krishnan (2009) have described military robots as “killer robots.” By  the  same  token,  human  soldiers  might be  called  “killers,”  or  even  “murderers.” However, it has long been disputed that soldiers should be described in this way. St.  Augustine,   for   example,   denied   that   soldiers   violated   the   commandment   Thou  shalt not kill:  “who is but the  sword in the hand of him who uses it, is not himself  responsible for the death he deals.” Those who act according to a divine command  or God’s laws as enacted by the state and who put wicked men to death “have by no  means violated the commandment, Thou shalt not kill” (St. Augustine, On the City of God). As  these  quotes  indicate,  in  military  ethics,  matters  are  not  as  simple  as  they  might  seem.  Calling  military  robots  “killer  robots”  brings  in  a  lot  of  background  assumptions.
To form a proper perspective on the ethics of the use of military robots, we need to  consider  the  ethics  of  war  and  peace.  “Just  war  theory”  is  probably  the  most





Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-27 21:37:18.
 
146                                                                                                                                          Chapter 9

inﬂuential perspective on the ethics of war and peace (Orend 2008). Just War Theory can be divided into three parts, which in the literature are referred to, for the sake of convenience, in Latin. These parts are:  (1) jus ad bellum, which concerns the justice of  resorting  to  war  in  the  ﬁrst  place;  (2) jus  in  bello,  which  concerns  the  justice  of conduct within war,  after it has begun;  and  (3) jus post bellum, which  concerns the justice of peace agreements and the termination phase of war. When discussing the deployment of military robots, jus in bello is clearly the most relevant category. Jus in bello refers to justice in war, to right conduct in the midst of battle. Responsibility for adherence to jus in bello norms falls primarily on the shoulders of those military com- manders, ofﬁcers, and soldiers who formulate and execute the war policy of a particu- lar state. They are to be held responsible for any breach of the principles that follow. It  is  common  to  distinguish between  external  and  internal jus  in  bello.  External,  or traditional, jus in bello concerns the rules a state should observe regarding the enemy and  its  armed  forces.  Internal jus  in  bello  concerns  the  rules  a  state  must  follow  in connection with its own people as it ﬁghts war against an external enemy. There are several rules of external jus in bello:
1.  Obey  all  international  laws  on  weapons  prohibition.  Chemical  and  biological weapons, in particular, are forbidden by many treaties.
2.  Discrimination and noncombatant immunity: soldiers are only entitled to use their (nonprohibited) weapons to target those who are “engaged in harm.” Thus, when they take aim, soldiers must discriminate between the civilian population, which is morally immune from  direct  and intentional  attack,  and those legitimate military, political, and industrial targets involved in rights-violating harm. While some collateral civilian casualties are excusable, it is wrong to take deliberate aim at civilian targets.
3.  Proportionality:  soldiers  may  only  use  force  proportional  to  the  end  they  seek. They must restrain their force to that amount appropriate to achieving their aim or target.
4.  Benevolent quarantine for prisoners of war: if enemy soldiers surrender and become captives they cease being lethal threats to basic rights. They are no longer  “engaged in harm.” Thus, it is wrong to target them with death, starvation, rape, torture, medical experimentation, and so on.
5.  No  means  that  are  mala  in  se:  soldiers  may  not  use  weapons  or  methods  that are  “evil  in  themselves.”  These  include:  mass  rape  campaigns,  genocide  or  ethnic cleansing, using poison, or treachery, forcing captured  soldiers to ﬁght against their own side, and using weapons whose effects cannot be controlled, such as biological agents.
6.  No reprisals: a reprisal is when country A violates jus in bello in war with country B. Country B then retaliates with its own violation of jus in bello, seeking to chasten A into obeying the rules.






Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-27 21:37:18.
 
Responsibility for Military Robots                                                                                                   147

Internal jus in bello essentially boils down to the need for a state, even though it’s involved in a war, nevertheless to still respect the human rights of its own citizens as best it can during the crisis.
What do these rules mean for military robots? They would behave unacceptably if they violated  at  least  one  of these  rules. We  may  distinguish between  two  types  of cases.  First,  let  us  assume  that  military  robots  are  nothing  but  “killer  robots”  (as Sparrow  [2007]  and  Krishnan  [2009]  seem  to  assume).  In  this  case,  they would  not necessarily be  immoral, because  they would  not  necessarily violate  one  or  more  of these rules. As long as their reactions were proportionate, not evil in themselves, only directed  toward  combatants,  and  so  on,  their behavior  could be  justiﬁable,  or  even praiseworthy.  Second,  let  us  assume  that  there  are  military  robots  that  are  not  just “killer robots,” but designed to avoid killing as much as possible. This is clearly a more attractive  option  than  the  ﬁrst  scenario.  It  was  brought  to  our  attention  when  we showed the following passage about the strength of innate moral emotions  (such as an aversion to killing) to a Dutch soldier (Chambers 2003):
These  innate  emotions  are  so powerful  that  they  keep people  moral  even  in  the  most  amoral  situations.  Consider the behavior of soldiers during war.  On the battleﬁeld, men are explicitly  encouraged to kill one another; the crime of murder is turned into an act of heroism. And yet,  even in such violent situations, soldiers often struggle to get past their moral instincts. During  World  War  II,  for  example,  U.S.  Army  Brigadier  General  SLA  Marshall  undertook  a  survey  of  thousands of American troops right after they’d been in combat. His shocking conclusion was  that less than 20 percent actually shot at the enemy, even when under attack. “It is fear of killing,” Marshall wrote, “rather than fear of being killed, that is the most common cause of battle failure  in  the  individual.”  When  soldiers  were  forced  to  confront  the  possibility  of  directly  harming  other human beings—this is a personal moral decision—they were literally incapacitated by their  emotions. “At the most vital point of battle,” Marshall wrote, “the soldier becomes a conscien-  tious objector.”
After these ﬁndings were published in 1947, the U.S. Army realized it had a serious problem.  It  immediately began  revamping  its  training  regimen  in  order  to  increase the “ratio of ﬁre.” New recruits began endlessly rehearsing the kill, ﬁring at anatomi- cally correct targets that dropped backward after being hit. As Lieutenant Colonel Dave Grossman  noted,  “what  is being  taught  in  this  environment  is  the  ability  to  shoot reﬂexively and instantly.  .  .  .   Soldiers are de-sensitized to the act of killing, until it becomes  an  automatic  response”  (Lehrer  2009).  The  army  also  began  emphasizing battleﬁeld  tactics,  such  as  high-altitude  bombing  and  long-range  artillery,  which managed to  obscure the personal  cost  of war. When bombs  are  dropped from forty thousand feet, the decision to ﬁre is like turning a trolley wheel: people are detached from  the  resulting  deaths. These  new  training  techniques  and  tactics  had  dramatic results.  Several years  after  he published  his  study, Marshall was  sent  to  ﬁght  in  the Korean War, and he discovered that 55 percent of infantrymen were now ﬁring their





Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-27 21:37:18.
 
148                                                                                                                                          Chapter 9

weapons. In Vietnam, the ratio of ﬁre was nearly 90 percent. The army had managed to turn the most personal of moral situations into an impersonal reﬂex. Soldiers no longer felt a surge of negative emotions when they ﬁred their weapons. They had been turned, wrote Grossman, into “killing machines” (Lehrer 2009,  173– 174).
Our military informant pointed out that, despite the appeal to military authority displayed in this passage  (“U.S. Army Brigadier General SLA Marshall”),1   the passage does notreﬂect current military practice at all. All military handbooks, atleast in the Netherlands, carefully point out that it is the aim of the military in battle to put the enemy  out  of  action,  to  neutralize  the  enemy  forces,  or  make  them  harmless—for example, by disarming them, immobilizing their vehicles, or turning them into pris- oners of war. Temporary incapacitation is preferable to killing. In fact, the handbooks avoid the term  “killing”  and view the  “elimination”  of enemy forces  as  a means  of last resort. Soldiers are taught to aim for the knees, not the heart or head, when taking target practice.
This is of the utmost importance in the context of autonomous intelligent military robots because they can be designed to immobilize or disarm enemy forces, instead of killing them. Because they can be equipped with superior sensory and incapacitat- ing devices  (and perhaps better decision circuitry as well, capable of better handling a greater amount of information more adequately than humans can do), they can in principle achieve this aim far more reliably than humans. In other words, it could be argued that autonomous robots are, in principle, morally superior to human soldiers, because the former could resort to temporary incapacitation in cases where the latter would  have  no  option  but  to  kill.  It  is  misleading  to  equate  autonomous  military robots with killer robots because it is quite possible that their deployment will save lives  instead  of  adding  to  human  loss.  Calling  them  “killer  robots”  is  an  insidious rhetorical move, which easily leads to a false dilemma. This brings us to our ﬁrst thesis:
Thesis  1. Artiﬁcially intelligent military robots that save lives are preferable to humans (or bombs) that kill blindly.

9.2   Responsibility, Punishment, and Blame

Suppose that something goes wrong on the battleﬁeld—that people get killed as the result of the action of an autonomous military robot instead of merely being put out of combat. Who is to blame in such a case—the robot itself, or its operator, program- mer, or designer?
Sparrow (2007) argues that robots cannot beheld responsible because they cannot be punished. They  cannot be punished because they  cannot  suffer.  In  other words, responsibility presupposes the ability to suffer. We want to object to this line of rea- soning for two reasons. First, it is by no means to betaken for granted that robots will






Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-27 21:37:18.
 
Responsibility for Military Robots                                                                                                   149

never be able to suffer. Second, punishment is not desirable in any case because we can use more effective means for adjustment in the case of robots that do not act in a desirable way.
First, it is  questionable that robots  cannot be made to  suffer.  On the  contrary, it has been argued that intelligent robots are bound to have emotions as the inevitable consequence of having motives and the processes they generate (Sloman and Croucher 1981). If robots can be made to suffer, then they can be punished, as well, so this part of Sparrow’s objection loses it force.
Second, let us grant that Sparrow is right and that robots cannot  suffer. We may then ask: what is the point of punishment, anyway? Its main justiﬁcation is the pre- vention of the type of behavior that brought it about. Punishment leads to suffering; humans  tend  to  avoid  suffering;  so  punishment  may  lead  to  prevention because  it gives humans a reason to avoid similar behavior in the future. What if an agent cannot suffer or cannot see that similar behavior in the future will again lead to harsh pun- ishment? Then we give them treatment. Treatment is another means to achieve pre- vention. When punishment is not an option, treatment remains. This not only applies to humans  (for example, mentally handicapped persons), but also to other types of agents. Cars cannot suffer, so we treat (repair, correct) them, simply because this may bring the desired goal (correct functioning in the future) closer.
In other words, it is important to make a distinction between the means and the ends.  Punishment  is  simply  one  means  that  may  lead  to  the  desired  end;  it  is  not desirable  in  itself.  If  other  courses  of  action  are  more  effective,  they  are  ipso  facto preferable.
This is important in the context of our military robots. If they cannot suffer, they cannot be punished. But it can be argued that punishment is not desirable anyway. It only detracts us from what really matters, namely the prevention of similar tragic actions in the future. It has been argued that men are nothing but machines. If so, similar considerations could be applied to them. It turns out that considerations along these lines can already be found in the literature. In a piece called “Let’s all stop beating Basil’s car,” Richard Dawkins (2006) wrote as follows:
Retribution  as  a  moral principle  is  incompatible with  a  scientiﬁc view  of human behavior. As scientists, we believe that human brains, though they may not work in the same way as man- made computers, are as surely governed by the laws of physics. When a computer malfunctions, we  do  not  punish  it.  We  track  down  the  problem  and  ﬁx  it,  usually by  replacing  a  damaged component, either in hardware or software.
Basil Fawlty, British television’s hotelier from hell, created by the immortal John Cleese, was at the end of his tether when his car broke down and wouldn’t  start. He gave it fair warning, counted to three, gave it one more chance, and then acted.  “Right! I warned you. You’ve had this coming to you!” He got out of the car, seized a tree branch and set about thrashing the car within an inch of its life. Of course, we laugh at his irrationality. Instead of beating the car, we





Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-27 21:37:18.
 
150                                                                                                                                          Chapter 9

would investigate the problem. Is the carburetor ﬂooded? Are the sparking plugs or distributor points damp? Has it simply run out of gas? Why do we not react in the same way to a defective man:  a  murderer,  say,  or  a  rapist?  Why  don’t  we  laugh  at  a  judge  who  punishes  a  criminal, just as heartily as we laugh at Basil Fawlty? Or at King Xerxes, who, in 480 BC, sentenced the rough sea to 300 lashes for wrecking his bridge of ships? Isn’t the murderer or the rapist just a machine with a defective component? Or a defective upbringing? Defective education? Defective genes?
When  Sparrow  laments  that  military  robots  cannot  be  held  responsible  because they cannot suffer, he resembles Basil Fawlty, who laments that his broken car does not respond to threats. Their reactions are misplaced for the same reasons. The alter- natives are clear in both cases as well: if you want to prevent such-and-such action, do something about it. If punishment does not help, adopt an alternative approach from among the courses of action that lead to more desirable behavior. In the case of humans, this means psychotherapy, chemical treatment, or neurosurgery and similar treatment; in the case of cars, this means looking at the carburetor, the sparking plugs, distributor points, and gas tank; in the case of nonhuman agents, this comes down to improving the sensory devices, ﬁne-tuning the response mechanisms, adjusting the nonmortal  combat  devices,  or  rewriting  the  software.  As  Dawkins   (2006)  wrote, “Assigning blame  and responsibility is  an  aspect  of the useful ﬁction  of intentional agents that we construct in our brains as a means of short-cutting a truer analysis of what is going on in the world in which we have to live.” It is a tremendous advantage, not a defect, that we do not have to assign blame and responsibility to robots, because we know what is going on inside them and what to do when something goes wrong. This brings us to our second thesis:
Thesis 2. It is regrettable and not satisfactory at all that punishment is usually the best we can do in the case of human wrongdoing.

9.3   The Logic of Responsibility

What  exactly  are  responsibility  and  agency?  In  recent years,  logicians  and  artiﬁcial intelligence  researchers  have  devoted  considerable  attention  to  this  topic  (Belnap, Perloff,  and  Xu  2001;  Horty  2001). The  literature  is vast  and  complicated, but  one thing to note is that logicians have made a distinction between two concepts of action:
1.  Seeing to it that (this is Chellas’s theory of CSTIT: Chellas’s Seeing To It That);
2.  Deliberatively seeing to it that (this is Horty’s theory of DSTIT: Deliberatively Seeing To It That).
DSTIT can be deﬁned in terms of CSTIT: an agent A deliberatively sees to it that P if and only if (1) A sees to it that P  (in the sense of Chellas) and  (2) it is possible that not P. Deliberative action presupposes the ability to make choices, the ability to do





Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-27 21:37:18.
 
Responsibility for Military Robots                                                                                                   151

otherwise;  agents’  choices usually  are  assumed  to be  independent  from  each  other. Chellas’s concept of seeing to it that does not depend on this notion of choice. CSTIT theory  is  a  theory  of  causal  responsibility,  while  DSTIT  theory  is  related  to  moral responsibility, because it is usually assumed than an agent should only beheld morally responsible if he or she could have done otherwise (this view goes back to Aristotle’s Nicomachean Ethics). It is to be noted that these analyses of responsibility do not mention punishment at all: this suggests that the concepts of responsibility and punishment are less closely related than Sparrow assumed.
How  does  this  apply  to  robots?  Let  us  ﬁrst  consider  the  case  of nondeliberating robots,  which  are  controlled  by  a  human  commander.  Such  cases  are  described  by sentences of the following form:
(1) Commander A deliberatively sees to it that robot B sees to it that P.
Who  is  responsible  in  such  a  case?  It turns  out  (as  a  matter  of logic) that both the robot and the commander are causally responsible, but it is only the commander (not the robot) that can be morally responsible, for the  simple reason that the robot has no choice and does not have the ability to do otherwise.
This is in perfect agreement with the legal maxims qui facit per alium facit per se and respondeat superior, which can be summarized as follows:
Qui facit per alium facit per se means “he who acts through another does the act himself.” This is a fundamental maxim of agency (Stroman Motor Co. v. Brown, 116 Okla 36, 243 P 133). A  maxim often stated in discussing the liability of employer for the act of employee  (35 Am J1st  M & S § 543). According to this maxim, if in the nature of things the master is obliged to perform  the  duties  by  employing  servants,  he  is  responsible  for  their  act  in  the  same  way  that  he  is  responsible for his own acts (Anno: 25 ALR2d 67).
Respondeat  superior  means  “let  the  master  answer.” This  is  a  legal  principle, which  states that, in most circumstances, an employer is responsible for the actions of employees performed within the course of their employment. This rule is also called the “Master-Servant Rule,” recog- nized in both common law and civil law jurisdictions. This principle is related to the concept of vicarious liability.
It is comforting to know that these age-old legal principles can be applied to modern robots and ﬂow naturally from our logical account.
The  analysis  is  more  complex  in  the  case  of  deliberative  (autonomous)  robots, which can potentially beheld responsible precisely because of their capacity to engage in deliberation. As noted earlier, agents’ choices are independent from each other, in the sense that any combination of possible choices available to different agents at the same  moment  must be  compatible.  Each  agent  can  choose  each  of its  alternatives, regardless  of what  the  other  agents  are  doing  at  the  moment. This  implies  that  an agent cannot deliberatively see to it that another agent deliberatively sees to it that something  is  the  case.  This  makes  this  case  quite  unlike  the  case  presented  in  the





Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-27 21:37:18.
 
152                                                                                                                                          Chapter 9

previous  section,  in which  an  agent  deliberatively  did  something by using  another agent  as  an  instrument.  One  cannot  exert  such  control  over  independent  agents; instead, we must think of other ways to induce them to perform in ways we see ﬁt. One simple way of doing so consists of blocking all undesirable courses of actions, in the sense of making them impossible; this undermines the subordinate agents’ ability to do otherwise, and leaves them no choice but to undertake the desired courses of action.
In general, even though an agent cannot see to it that another agent makes a certain speciﬁc choice (the latter agent can always choose differently), an agent can see to it that another agent makes some choice. Formally: even though
(1) Commander A deliberatively sees to it that robot B deliberatively sees to it that P is necessarily false,
(2) Commander A deliberatively sees to it that: either robot B deliberatively sees to it that P or robot B deliberatively sees to it that not P
might well be true.
Situations  of  the  latter  type  have  been  called  situations  of  “forced  choice”  (Belnap Perloff, and Xu 2001, chapter 10B2). We may similarly speak of cases of “forced moral responsibility.”
Cases  of this type  have played  a prominent  role  in  military trials  (Wikipedia.org 2010). Nuremberg Principle IV states “the fact that a person acted pursuant to order of  his  government  or  of  a  superior  does  not  relieve  him  from  responsibility  under international law, provided a moral choice was in fact possible to him.” Similarly, in the Ehren Watada case, the judge ruled that soldiers, in general, are not responsible for determining whether the order to go to war itself is a lawful order—but are only responsible for those orders resulting in a speciﬁc application of military force, such as  an  order  to  shoot  civilians,  or  to  treat  POWs  inconsistently  with  the  Geneva Conventions. Nuremberg Principle IV and the Ehren Watada judgment concern the choices and moral responsibility of agents in  situations that were brought about by other agents (their superiors).
Even though logicians and lawyers can reason about cases in which forced choices play  a  role,  it  is  doubtful  whether  such  situations  will  play  a  role  in  robot  ethics. Autonomous military robots that deliberate and perform voluntary actions out of their own accord seem very far off indeed. They might even be seen as unwelcome in view of the risk of insubordination; commanders might object to robots that protest against their commanders’ or operators’ commands. The case of nondeliberative robots that are used as instruments by their operators seems more realistic. We discussed this case earlier (referring to the qui facit per alium and respondeat superior principles) and in fact came to a similar conclusion as St. Augustine did (section 9.1).






Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-27 21:37:18.
 
Responsibility for Military Robots                                                                                                   153

9.4   Design of Military Robots

Even  though  intelligent  military  robots  may  turn  out  to  be  morally  preferable  to humans for the reasons that we have indicated,2   this  does not mean that it will be easy to build them. Quite apart from the technical aspects (superior sensory devices, discrimination  between  friend  and  foe,  and  so  on),  there  are  ethical  questions  to consider. For example, should ethical principles (do not kill unnecessarily, avoid col- lateral damage, do not harm civilians, do not torture, respect the Geneva Conventions, and so on) be included in their lists of goals to pursue, or pitfalls to be avoided? But what do these principles mean exactly? How can they be made precise? For example, what is torture, anyway? How can it be demarcated from mild pressure? Is a civilian who  supports  the  enemy  an  enemy?  A  lot  of  conceptual  ethical  analysis  is  needed before such principles have been made precise enough to such a degree that they can be burnt into the hardware or software that controls the behavior.
Furthermore,  how  should  they be built  in?  Is  ethics  primarily  a  matter  of logic? Should robots follow these rules by means of logical reasoning, namely, by proving theorems and refuting nontheorems? (Bringsjord, Arkoudas, and Bello 2006; van den Hoven and Lokhorst 2002). If so, should default reasoning perhaps be built in, should the  frame  problem  (including  the  moral  frame  problem) be  considered,  and  should the problem of induction and abduction be solved before we set out on this path? Is some kind of self-monitoring, a module that keeps track of the robot’s moral reason- ing, worth building in (Lokhorst forthcoming)? Or should we forget about logic and merely build in appropriate pattern recognition software, perhaps in the form of sta- tistical software or neural networks? Or better yet, should both routes be pursued, just as in the case of humans, who are often asserted to have two decision mechanisms, a fast, automatic, innate mechanism, which provides us with our gut feelings, and a slow, conscious, learned circuit, which takes care of our rational decisions?3  If so, how should they be kept in balance? Is it necessary to incorporate a mechanism that keeps track of actions that should have been done otherwise (i.e., a mechanism that gener- ates regret)?
Nobody knows at this moment, and much research is needed before we will be able to answer these questions. Before we embark on such research, we should try to answer the preliminary question of whether its objective is ethically desirable. We have tried to answer this question in this chapter. According to us, there can be no doubts about its proper answer. We therefore propose our third thesis:
Thesis 3. From a moral point of view, the design of military robots is eminently     desirable, provided that such robots are designed as transparent robots that avoid killing to the maximum extent possible, and not as inscrutable killer robots, over which we have no control.





Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-27 21:37:18.
 
154                                                                                                                                          Chapter 9

Even if military robots could beheld responsible to some extent (as discussed earlier in the forced choice cases), this would never excuse us in case something goes wrong, because those who design and deploy military robots are those who are responsible for them in the ﬁrst place (as indicated by the qui facit per alium and respondeat superior principles previously discussed). This may be regarded as unfortunate, but we regard it as welcome because we have more control over the design of military robots that act in agreement with our own ethical speciﬁcations than over the training of human soldiers, which is a hit-and-miss affair, at best.

9.5   Conclusion

We claim that it should never be assumed that human beings, in their role of designer, maker,  manager,  or user  of robots  and  other  artifacts  or  technological  systems,  can transfer moral responsibility to their products in case of untoward outcomes, or can claim diminished responsibility for the consequences brought about by their products. We claim that designers of autonomous robots are  “design responsible” in all cases. In the causal case, this is so for the reasons we have expounded, since the robot is an instrument like any other artifact. In the deliberative case, it is so because the designer is responsible for “designing in” the logic of deontic reasoning and deontic metarea- soning,  which  will  lead  the  robot  to  make  the  right  choices  similar  to  the  way  in which we teach  our  children to think correctly in moral matters.  In both  cases, we think it would be unethical to produce such systems or work on their development while assuming that the locus of full and undivided responsibility for outcomes can be assigned to the artifacts themselves, however accomplished and sophisticated they are. We consider the shift of responsibility to the thing one has produced as an ulti- mate form of bad faith, meaning, denial of human choice, freedom, and responsibility. The  designers,  producers,  managers,  overseers,  and  users  are  and  remain  always responsible. The fact that it is difﬁcult to apportion responsibility should not deter us. The apportioning of responsibility outside the simplest cases is problematic anyway. We hope that this contribution will make it easier to allocate responsibility adequately and fairly when thinking about responsibility and robots.
We focus on the responsibility of the designers and refer to their speciﬁc responsi- bility as “design responsibility.” One speciﬁc and important aspect of design respon- sibility  is  to  design  in  accordance  with  well-accepted  and  widely  shared  values.  In software engineering, this approach is referred to as “value sensitive design” (van den Hoven and Manders-Huits 2009). In the case of military robots, there is a well-accepted normative  framework  in  the  form  of  Geneva  Conventions,  laws  of war,  and,  more generally,  the  doctrines  of  just  war  theory—jus  ad  bellum,  in  bello,  and post bellum. These provide us with moral principles that need to be translated and applied to the design of military robots. These principles are fairly broad since they also pertain to





Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-27 21:37:18.
 
Responsibility for Military Robots                                                                                                   155

the design of the institutional context that guarantees design compliance with these accepted doctrines and their implications.
But,  as  former  Pentagon  Chief  of  High  Value  Targeting  Marc  Garlasco  said,  we cannot  simply  download  international  law  into  a  computer   (Singer  2009,  389). Sustained legal engagement and ethical reﬂection must be present from the very begin- ning of the design process. Investigating how ethical and legal norms can be “designed in” to complex systems is a core research goal of this process.

Notes

1.  The  ﬁndings  of military  writer  and  analyst  S.  L.  A.  Marshall,  syndicated  columnist  for  the Detroit News and brigadier general in the Army Reserve, are less reliable than is usually reported: see Chambers 2003.
2.  Arkin has made a similar claim: “My research hypothesis is that intelligent robots can behave more  ethically in the battleﬁeld than humans  currently  can. That’s the  case  I make”  (cited in Dean 2008; see also Arkin 2009).
3.  See the book by Lehrer for a description of these two mechanisms, their strengths and weak- nesses, and a discussion of the question when to use which of the two. Also see the discussion about the necessity of merging the cognitive top-down approach with a less cognitive bottom-up approach (Wallach and Allen 2009).

References

Arkin, Ronald. 2009. Governing Lethal Behavior: Embedding Ethics in a Hybrid Deliberative/Reactive Robot Architecture. Boca Raton, FL: CRC Press.
Belnap, Nuel, Michael Perloff, and Ming Xu. 2001. Facing the Future: Agents and Choices  in  Our Indeterminist World. New York: Oxford University Press.
Bringsjord, Selmer, Konstantine Arkoudas, and Paul Bello. 2006. Toward a general logicist meth- odology for engineering ethically correct robots. Intelligent Systems 21 (4): 38–44.
Chambers, John Whiteclay, Jr. 2003. S. L. A. Marshall’s Men Against Fire: New evidence regarding ﬁre ratios. Parameters (Autumn):  113– 121.
Dawkins, Richard. 2006. Let’s all stop beating Basil’s car. The World Question Center. <http://www .edge.org/q2006/q06_9.html> (accessed March 26, 2011).
Dean,  Cornelia.  2008.  A  soldier,  taking  orders  from  its  ethical  judgment  center.  The New  York Times, November 24.
Horty, John F. 2001. Agency and Deontic Logic. New York: Oxford University Press.
Krishnan, Armin. 2009. Killer Robots: Legality and Ethicality of Autonomous Weapons. Farnham and Burlington, VT: Ashgate.




Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-27 21:37:18.
 
156                                                                                                                                          Chapter 9

Lehrer, Jonah. 2009. The Decisive Moment. Edinburgh: Canongate. (Originally published as How We Decide. New York: Houghton Mifﬂin Harcourt.)
Lokhorst,   Gert-Jan   C.   Forthcoming.   Computational   meta-ethics:   Towards   the   meta-ethical robot. Minds and Machines. <http://www.springerlink.content/d819182mwk4u0146/fulltext.pdf> (accessed July  14, 2011).
Orend, Brian. 2008. War.  The Stanford Encyclopedia of Philosophy (Fall ed.), ed. Edward N. Zalta. Metaphyics Research Lab,CSLI, Stanford University. <http://plato.stanford.edu/archives/fall2008/ entries/war/> (accessed November 20, 2010).
Singer, Peter Warren. 2009. Wired for War: The Robotics Revolution and Conﬂict in the 21st Century. New York: Penguin Press.
Sloman, Aaron, and Monica Croucher. 1981. Why robots will have emotions. In Proceedings IJCAI 1981  Vancouver, ed. P. J. Hayes,  197–202. Los Altos, CA: William Kaufmann.
Sparrow, Robert. 2007. Killer robots. Journal of Applied Philosophy 24 ( 1): 62– 77.
van den Hoven, Jeroen, and Gert-Jan C. Lokhorst. 2002. Deontic logic and computer-supported computer ethics. Metaphilosophy 33 (3): 376–386.
van  den  Hoven,  Jeroen,  and  N.  L.  J.  L.  Manders-Huits.  2009.  Value-sensitive  design.  In  A Companion to the Philosophy of Technology, ed. J. K. Berg Olsen, S.A. Pedersen, and V. Hendricks, 477–480. Chichester, UK: Wiley-Blackwell.
Wallach,  Wendell,  and  Colin  Allen.  2009.  Moral  Machines:  Teaching Robots  Right  from  Wrong. Oxford: Oxford University Press.
Wikipedia.org.  2010.  Superior  orders.  <http://en.wikipedia.org/wiki/Superior_orders>  (accessed July 14, 2011).







The Rights and Wrongs of Robot Care                                                                                          273

it could be equivalent to imprisonment in the home without trial. There are already circumstances in which carers can restrict the liberty of individuals in order to protect them.  However,  there  are  legal  procedures  available  for  making  such  decisions.  We must ensure that we do not let the use of technology covertly erode the right to liberty without due process.
17.2   Human Contact and Socialization

It is the natural right of all individuals to have contact with other humans and social- ize freely. If robots begin to be trusted to monitor and supervise vulnerable members of  society,  and  to  perform  tasks  such  as  feeding, bathing,  and  toileting,  a  probable consequence is that some young and old humans could be left in the near-exclusive company of robots.
In discussing the effect of new therapies for people with aging brains, Boas (1998) points out, “What stimulates them, gives a lift to their spirits, is the human interac- tion, the companionship of fellow human beings.” And having a good social network helps  to  protect  against  declining  cognitive  functions  and  incidence  of  dementia (Crooks,  Lubben,  and  Petitti  2008;  Bennett  et  al.  2006).  For  children,  very  serious defects both in brain development and psychological development can occur if they are deprived of human care and attention (Sharkey and Sharkey 2010a). The effects, and risks, of reduced human contact are likely to be quite different for the elderly and for infants. Infants need nurturing and parenting to enable their normal development, while  the  elderly  require  companionship  to  avoid  loneliness  and  to  maintain  their mental health for longer. We will deal with each of these populations separately.
17.2.1   First Contact with the Robots: Infants in Care
The  impairments  caused  by  extreme  lack  of  human  contact  with  infants  are  well known  and  documented. Nelson  and  colleagues  (Nelson  et  al.  2007)  compared  the cognitive  development  of young  children  reared  in  Romanian  institutions  to  those moved to foster care with families. Children reared in institutions manifested greatly diminished  intellectual  performance  (borderline  mental  retardation)  compared  to children reared in their original families. Chugani and colleagues (Chugani et al. 2001) found that Romanian orphans, who had experienced virtually no mothering, differed from  children  of  comparable  ages  in  their  brain  development—and  had  less  active orbitofrontal cortex, hippocampus, amygdala, and temporal areas.
Perhaps little or no harm would result from a child being left in the care of a robot for very short periods. But what would happen if those periods of time became increas- ingly frequent and longer? The outcome would clearly depend on the age of the child in question. It is well known that infants under the age of two need a person with whom they can form an attachment if they are to develop well. In an earlier paper (Sharkey and Sharkey 2010a), we considered whether an infant might be able to form




Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-28 20:42:05.
 
274                                                                                                                                        Chapter 17

an  attachment  to  a  robot  caregiver,  perhaps  in  the  same  way  that  Harry  Harlow’s monkeys became attached to a static cloth surrogate mother.
What  research  there  is  suggests  that  very  young  children  can  form  bonds  with robots. Tanaka, Cicourel, and Movellan (2007) placed a “state-of-the-art” social robot (QRIO, made by Sony), in a daycare center for ﬁve months. They found that the tod- dlers (aged between ten and twenty-four months) bonded and formed attachments to the QRIO robot in a way that was signiﬁcantly greater than their bonding with a teddy bear. They touched the robot more than they hugged or touched a static toy robot, or  a  teddy  bear.  The  researchers  concluded,  “Long-term  bonding  and  socialization occurred between toddlers and the social robot.”
Turkle and colleagues (Turkle et al. 2006a) report a number of individual case studies that attest to children’s willingness to become attached to robots. For example, ten- year-old Melanie describes her relationship with the robotic doll “My Real Baby” that she took home for several weeks:
Researcher:    Do  you  think  the  doll  is  different  now  than  when  you  ﬁrst   started  playing with it?
Melanie:    Yeah. I think we really got to know each other a whole lot better. Our relationship, it grows bigger. Maybe when I ﬁrst started playing with her, she didn’t really know me so she wasn’t making as much  [sic] of these noises, but now that she’s played with me a lot more, she really knows me and is a lot more outgoing. (Turkle et al. 2006a, 352)
In another paper, Turkle and colleagues (Turkle et al. 2006b) chart the ﬁrst encoun-  ters of sixty children between the ages of ﬁve and thirteen with the MIT robots Cog  and  Kismet.  The  children  anthropomorphized  the  robots,  made  up  “back  stories” about their behavior, and developed “a range of novel strategies for seeing the robots  not only as ‘sort of alive’ but as capable of being friends and companions.” Their view  of the robots did not seem to change when the researchers spent some time showing  them  how  they worked,  and  emphasizing  their underlying  machinery.  Melson  and  colleagues (Melson et al. 2009) directly compared children’s views of and interactions  with  a  living  dog  and  a  robot  dog  (AIBO).  Although  there  were  differences,  the  majority of the children interacted with the AIBO in ways that were like interacting  with  a real  dog: they were  as likely to give  commands to the AIBO  as to the living  dog, and over 60 percent afﬁrmed that AIBO had “mental states, sociality, and moral  standards.”
Overall, the pattern of evidence indicates that children  saw robots that they had spent  time  with  as  friends  and  felt  that  they  had  formed  relationships  with  them. They even believed that a relatively simple robot was getting to know them better as they played with it more. So, extrapolating from the evidence, it seems that there is a good possibility that children left in the care of robots for extended periods could form  attachments  to  them.  However,  it  is unlikely  that  the  attachment would  ade- quately replace the necessary support provided by human attachment.





Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-28 20:42:05.
 
The Rights and Wrongs of Robot Care                                                                                          275

To become well adjusted and socially attuned, an infant needs to develop a secure attachment to a carer (Ainsworth, Bell, and Stayton 1974). A securely attached infant will explore their environment conﬁdently, and be guided in their exploration by cues from the  carer. The  development  of secure  attachment between  a human  carer  and an  infant  depends  on  the  carer’s  maternal  sensitivity,  and  ability  to  perceive  and understand  the  infant’s  cues  and  to  respond  to  them  promptly  and  appropriately. Detailed interactions between a mother and baby help the infant to understand their own emotions, and those of others.
In Sharkey and Sharkey  (2010a), we argued from a review of the technology that robot carers into the foreseeable future would be unable to provide the detailed inter- action necessary to replace human sensitivity and promote healthy mental develop- ment. Many aspects of human communication are beyond the capabilities of robots. There has been progress in developing robots and software that can identify emotional expressions (e.g., Littlewort, Bartlett, and Lee 2009) and there are robots that can make emotional expressions (Breazeal 2002; Cañamero and Fredslund 2001). However, rec- ognizing what emotion is being expressed is only a tiny  step toward understanding the causes of the emotion—is the child crying because she dropped her toy, because she is in pain, or because her parents are ﬁghting?
There are many challenges to be overcome to develop a robot that could respond appropriately  and  sensitively to  a young  child that  currently  seem insurmountable. This  is  further  complicated  because  responses  that  may  be  appropriate  at  one  age would  not  be  appropriate  at  another.  An  important  function  of  a  caregiver  is  to promote  a  child’s  development,  for  instance,  by  using  progressively  more  complex utterances in tune with the child’s comprehension.
When a human carer is insufﬁciently sensitive, insecure attachment patterns can result:  anxious-avoidant attachment when  the  child  frequently  experiences  rejection from  the  carer;  anxious  ambivalent attachment  when  the  carer  is  aloof  and  distant; disorganized attachment when there is no consistency of care and parents are hostile and frightening to the children. Babies with withdrawn or depressed mothers are more likely  to  suffer  aberrant  forms  of  attachment:  avoidant  or  disorganized  attachment (Martins and Gaffan 2000).
Perhaps a child with a secure attachment to their parent would not suffer much as a result of being left with a robot for short periods. But the fact is we just don’t know: no one has yet researched the possible negative consequences of children being left with robots for varying time periods, and it would be too risky to do so. We do know that young children do best when they spend time with a caregiver with whom they have a secure attachment. Thus, it is highly likely that leaving children in the care of a robot is not going to beneﬁt them as much as leaving them in the care of an atten- tive  and  focused  human  carer.  Robot  nannies  should  not  be  used  just  because  we cannot demonstrate that they are harmful. Rather, they should “qualify for (part-time)





Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-28 20:42:05.
 
276                                                                                                                                        Chapter 17

care only when it is proven that their use serves the child’s best interests”  (Zoll and Spielhagen 2010, 298).

17.2.2   Human Contact and the Elderly
A major concern that we have about home robot care for the elderly is that it may replace human  contact. With very  advanced  smart  sensing  systems  and robots that can lift and carry, bathe and feed, as well as keep their charges safe, there will be less need for care visits—the whole point of using the robots is because there will be fewer carers available as the population ages. This is bad news for many elderly people for whom visiting carers are the only human companionship they have on a daily basis. According  to  a  report  from  the  charity  Help  the Aged  in  2008,  17  percent  of older people in the UK have less than weekly contact with family, friends, and neighbors, and  11 percent have less than monthly contact.
Using robots for care of the elderly seems likely to reduce the number of opportuni- ties they have for interaction with other human beings, and the beneﬁts that come from such interaction. Sparrow and Sparrow  (2006) argue that robots should not be used in elder care because of the likely consequential reduction in social contact. They make the point that even using robots to clean ﬂoors removes a valuable opportunity for interaction between an elderly resident and a human cleaner.
Research strongly suggests human companionship is essential for the well-being of the elderly, and yet there are no speciﬁc rights to companionship. There is a right to participation in the culture in Article 27 of Universal Declaration of Human Rights.2  Deprivation of human contact may also be considered as cruelty, which is covered by Article  5.  However,  it  is  not  clear  that  someone  living  independently  in  their  own home  with  the  help  of  robots  would  be  being  subjected  to  lack  of  companionship. Home  helpers  are  not  employed  speciﬁcally  as  companions;  it  is  just  one  of  their beneﬁcial side effects. Before introducing mass robot care, this side effect needs to be recognized as a function. Substantial evidence suggests that human contact should be seen as part of the right to welfare and medical treatment.
It  is  clear  that  an  extensive  social  network  offers  protection  against  some  of the effects of aging: being single and living alone is a risk factor for dementia (Fratiglioni et al. 2000; Saczynski et al. 2006; Wilson et al. 2007). Holtzman et al.  (2004) found that frequent interaction in larger social networks was positively related to the main- tenance of global cognitive function. Wang et al. (2002) similarly found evidence that a rich  social network may decrease the risk of developing dementia, and concluded that  both  social  interaction  and  intellectual  stimulation  play  an  important  role  in reducing such risks.
There is evidence that stress exacerbates the effects of aging (Smith 2003),and that social contact can reduce the level of stress a person experiences. Kikusui, Winslow, and Mori (2006) provide a wide-ranging review of the phenomena of social buffering,





Lin, P., Abney, K., & Bekey, G. A. (Eds.). (2011). Robot ethics : The ethical and social implications of robotics. MIT Press.
Created from socal on 2023-02-28 20:42:05.
 
The Rights and Wrongs of Robot Care                                                                                          277

whereby  highly  social  mammals  show  better  recovery  from  distress  when  in  the company of conspeciﬁcs. A recent review (Heinrichs, von Dawans, and Domes 2009) concludes that the stress-protective effects of social support may be the result of the neurotransmitter oxytocin that is released in response to positive social interactions, and that oxytocin can have the effect of reducing stress.
One  take  on  the  problem  of  social  exclusion  of the  elderly  in Japan  is  to  move toward the development of robot companions and robot pets. These are being touted as a solution to the contact problem—devices that can offer companionship, entertain- ment,  and  human-like  support.  Examples  include  Paro,  a  fur-covered  robotic  seal developed  by  AIST  that  responds  to  petting;  Sony’s  AIBO  robotic  dog;  NeCoRo (OMRON), a robotic cat covered in synthetic fur, and My Real Baby (iRobot), described as an “interactive emotionally responsive doll.”
There  are,  to  our  knowledge,  no  studies  that  directly  compare  the  effect  on  the elderly of robot versus human companionship. Obviously, as is the case with children, robots are not going to be able to be as responsive to the needs of the elderly as are humans.  However,  they  might be useful  to  supplement  rather  than  replace  human carers. There is, for example, evidence that giving the elderly robot pets to look after can be beneﬁcial. Positive effects, such as reduction in loneliness and improved com- munication, have been found in studies where elders were allowed to interact with a simple Sony AIBO robot dog (Kanamori, Suzuki, and Tanaka 2002; Banks, Willoughby, and Banks 2008; Tamura et al. 2004).
These outcomes need to be interpreted with caution, as they depend on the alter- natives on offer. If the alternative is being left in near-complete social isolation, it is unsurprising that interacting with a robot pet offers advantages. Better comparisons could be made such as with a session of foot massage, or sitting with a sympathetic human listener.
On the upside, a robot pet does not have to be a replacement for social interaction. It could be provided in addition to other opportunities, and might further improve the well-being of an elderly person. As discussed in Sharkey and Sharkey (2010b), robot pets and toys could act as facilitators for social interaction by providing conversational opportunities (Kanamori, Suzuki, and Tanaka 2002). Having a robot pet may also give elders  an  increased  feeling  of control  and  autonomy. There  is  strong  evidence  that these factors can improve their well-being, and even result in longer life expectancy (Langer and Rodin  1976).
